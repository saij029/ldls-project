data:
  dataset_type: "sft"
  train_path: "data/processed/sft/train"
  val_path: "data/processed/sft/val"
  num_workers: 4
  preprocessing_num_workers: 8

training:
  output_dir: "models/checkpoints/sft"
  num_train_epochs: 3
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 8
  gradient_accumulation_steps: 8
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  max_grad_norm: 1.0
  
  logging_steps: 10
  eval_steps: 200
  save_steps: 200
  save_total_limit: 3
  
  fp16: false
  bf16: true
  
  optim: "adamw_torch"
  seed: 42

monitoring:
  wandb_project: "llm-code-trainer"
  use_tensorboard: true
  log_gpu_metrics: true
  gpu_metrics_interval: 10
