data:
  dataset_type: "cpt"
  train_path: "data/processed/cpt/train"
  val_path: "data/processed/cpt/val"
  num_workers: 4
  preprocessing_num_workers: 8

training:
  output_dir: "models/checkpoints/cpt"
  num_train_epochs: 3
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 16
  gradient_accumulation_steps: 4
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_ratio: 0.05
  lr_scheduler_type: "cosine"
  max_grad_norm: 1.0
  
  logging_steps: 10
  eval_steps: 500
  save_steps: 500
  save_total_limit: 3
  
  fp16: false
  bf16: true
  
  optim: "adamw_torch"
  seed: 42

monitoring:
  wandb_project: "llm-code-trainer"
  use_tensorboard: true
  log_gpu_metrics: true
  gpu_metrics_interval: 10
